chatgpt_detector/
├── model/
│   ├── Dockerfile  # Dockerfile for training the model
│   └── train.py    # Script for training the model
├── inference/
│   ├── Dockerfile  # Dockerfile for running inference using the trained model
│   └── app.py      # Flask or Django app for serving the inference API
├── terraform/
│   ├── main.tf     # Terraform configuration for creating AWS infrastructure
│   ├── variables.tf    # Variables tfile for Terraform
│   └── ...
├── README.md
└── requirements.txt  # Python dependencies for the project


docker build -t chatgpt-train -f model/Dockerfile .

docker build -t chatgpt-infer -f inference/Dockerfile .
docker build -t chatgpt-infer -f inference/Dockerfile --build-arg AWS_ACCESS_KEY_ID --build-arg AWS_SECRET_ACCESS_KEY .

docker run -it -e AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id --profile default) -e AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key --profile default) chatgpt-train

docker run -it -e AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id --profile default) -e AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key --profile default) chatgpt-infer

```
import requests

# Define the API endpoint URL
url = "http://localhost:5000/predict"

# Define the input text for prediction
input_text = "This is a test input."

# Create the request payload
payload = {"input_text": input_text}

# Send a POST request to the API endpoint
response = requests.post(url, json=payload)

# Check if the request was successful (status code 200)
if response.status_code == 200:
    # Extract the JSON response
    prediction = response.json()

    # Print the prediction
    print("Input Text:", prediction["input_text"])
    print("Predicted Class:", prediction["predicted_class"])
else:
    print("Error:", response.text)
```

Result

```
Input Text: This is a test input.
Predicted Class: 0
```